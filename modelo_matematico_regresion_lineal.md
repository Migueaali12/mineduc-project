# ğŸ”¬ Modelo MatemÃ¡tico de RegresiÃ³n Lineal MÃºltiple

## LinearRegression() de Scikit-Learn

Basado en el anÃ¡lisis del cÃ³digo donde se implementa:
```python
modelo_final = LinearRegression()
modelo_final.fit(X_train_scaled, y_train)
```

---

## ğŸ“ EcuaciÃ³n Fundamental

El modelo implementa la **regresiÃ³n lineal mÃºltiple** con la siguiente ecuaciÃ³n:

```math
Å· = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚šxâ‚š
```

**Donde:**
- `Å·` = Variable predicha (ICE - Ãndice de Calidad Educativa)
- `Î²â‚€` = Intercepto (constante)
- `Î²â‚, Î²â‚‚, ..., Î²â‚š` = Coeficientes de regresiÃ³n
- `xâ‚, xâ‚‚, ..., xâ‚š` = Variables predictoras (caracterÃ­sticas institucionales)

---

## ğŸ¯ FormulaciÃ³n Matricial

En notaciÃ³n matricial:

```math
ğ² = ğ—ğ›ƒ + ğ›†
```

**Componentes:**
- `ğ²` = Vector de valores objetivo (n Ã— 1)
- `ğ—` = Matriz de caracterÃ­sticas (n Ã— p+1, incluye columna de 1s para intercepto)
- `ğ›ƒ` = Vector de coeficientes ((p+1) Ã— 1)
- `ğ›†` = Vector de errores (n Ã— 1)

---

## âš¡ MÃ©todo de OptimizaciÃ³n: MÃ­nimos Cuadrados Ordinarios (OLS)

### FunciÃ³n de Costo

El algoritmo minimiza la **funciÃ³n de pÃ©rdida cuadrÃ¡tica**:

```math
J(ğ›ƒ) = (1/2m) Î£áµ¢â‚Œâ‚áµ (h_ğ›ƒ(ğ±â½â±â¾) - yâ½â±â¾)Â²
```

En forma matricial:
```math
J(ğ›ƒ) = (1/2m) ||ğ—ğ›ƒ - ğ²||Â²
```

### SoluciÃ³n AnalÃ­tica - EcuaciÃ³n Normal

Los coeficientes se calculan usando:

```math
ğ›ƒ = (ğ—áµ€ğ—)â»Â¹ğ—áµ€ğ²
```

Esta es la **soluciÃ³n exacta** que minimiza la suma de errores cuadrÃ¡ticos.

---

## ğŸ”® PredicciÃ³n para Nuevas Observaciones

Para realizar predicciones:

```math
Å·_nuevo = ğ±áµ€_nuevo ğ›ƒ
```

---

## ğŸ“Š AplicaciÃ³n en tu Contexto EspecÃ­fico

Con las **15 variables predictoras** identificadas en el modelo:

```math
ICE = Î²â‚€ + Î²â‚Â·AÃ±o_encoded + Î²â‚‚Â·Canton_encoded + Î²â‚ƒÂ·Sostenimiento_encoded 
    + Î²â‚„Â·Tipo_Educacion_encoded + Î²â‚…Â·Regimen_encoded + Î²â‚†Â·Jurisdiccion_encoded
    + Î²â‚‡Â·Area_encoded + Î²â‚ˆÂ·Modalidad_encoded + Î²â‚‰Â·Jornada_encoded
    + Î²â‚â‚€Â·Total_Estudiantes + Î²â‚â‚Â·Total_Docentes + Î²â‚â‚‚Â·Estudiantes_Femenino
    + Î²â‚â‚ƒÂ·Docentes_Femenino + Î²â‚â‚„Â·Docentes_Masculino + Î²â‚â‚…Â·Estudiantes_Masculino
```

---

## âœ… Suposiciones del Modelo

1. **ğŸ”— Linealidad**: RelaciÃ³n lineal entre predictoras y variable objetivo
2. **ğŸ”„ Independencia**: Observaciones independientes entre sÃ­
3. **ğŸ“ Homocedasticidad**: Varianza constante de los errores
4. **ğŸ“ˆ Normalidad**: Errores siguen distribuciÃ³n normal
5. **âŒ No multicolinealidad**: Variables predictoras no perfectamente correlacionadas

---

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

### RÂ² (Coeficiente de DeterminaciÃ³n)
```math
RÂ² = 1 - (SS_res/SS_tot) = 1 - (Î£áµ¢(yáµ¢ - Å·áµ¢)Â²)/(Î£áµ¢(yáµ¢ - È³)Â²)
```

### MAE (Error Absoluto Medio)
```math
MAE = (1/n) Î£áµ¢â‚Œâ‚â¿ |yáµ¢ - Å·áµ¢|
```

### RMSE (RaÃ­z del Error CuadrÃ¡tico Medio)
```math
RMSE = âˆš[(1/n) Î£áµ¢â‚Œâ‚â¿ (yáµ¢ - Å·áµ¢)Â²]
```

---

## ğŸ¯ Resultados en tu Modelo

| MÃ©trica | Valor | InterpretaciÃ³n |
|---------|-------|----------------|
| **RÂ²** | 0.100 | Explica el 10% de la variabilidad en ICE |
| **MAE** | 4.87 | Error promedio de Â±4.9 puntos de ICE |
| **RMSE** | 7.84 | DesviaciÃ³n estÃ¡ndar del error |

---

## ğŸ”§ Proceso de ImplementaciÃ³n

### 1. Preprocesamiento
```python
# NormalizaciÃ³n de variables numÃ©ricas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_numeric)

# CodificaciÃ³n de variables categÃ³ricas
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_encoded = le.fit_transform(X[col])
```

### 2. Entrenamiento
```python
modelo_final = LinearRegression()
modelo_final.fit(X_train_scaled, y_train)
```

### 3. PredicciÃ³n
```python
y_pred = modelo_final.predict(X_test_scaled)
```

---

## ğŸ’¡ CaracterÃ­sticas del Algoritmo

- **Tipo**: Supervisado, RegresiÃ³n
- **Complejidad**: O(pÂ³) para inversiÃ³n de matriz + O(npÂ²) para multiplicaciÃ³n
- **Ventajas**: 
  - SoluciÃ³n exacta (no iterativa)
  - Interpretable
  - RÃ¡pido para datasets medianos
- **Limitaciones**:
  - Asume relaciones lineales
  - Sensible a outliers
  - Problemas con multicolinealidad alta

---

## ğŸ§® Detalles Computacionales

Scikit-learn utiliza optimizaciones como:
- **DescomposiciÃ³n SVD** para mayor estabilidad numÃ©rica
- **NormalizaciÃ³n automÃ¡tica** para evitar problemas de escala
- **Manejo robusto** de matrices singulares

El algoritmo es determinÃ­stico y siempre converge a la misma soluciÃ³n Ã³ptima global. 